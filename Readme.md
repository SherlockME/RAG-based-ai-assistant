# ğŸ§  RAG-Based AI Teaching Assistant 

This project builds a **Retrieval-Augmented Generation (RAG)** pipeline that acts as a smart **AI Teaching Assistant** for a Python course.  
It uses automatic speech recognition (ASR), embeddings, and vector search to retrieve relevant transcript segments from long-form video lectures â€” and can later be extended to generate answers using an LLM.

---

## ğŸ¯ Objective
To create an intelligent assistant that answers course-related questions by:
1. Converting lecture videos â†’ audio â†’ transcripts  
2. Storing all transcript chunks as semantic embeddings  
3. Retrieving top-matching chunks for a userâ€™s query  
4. (Optionally) Sending those chunks to an LLM for contextual answers  

---

## ğŸ§© Tech Stack

| Stage | Tool / Library | Purpose |
|-------|----------------|----------|
| Transcription | **OpenAI Whisper (large-v2 / base)** | Converts audio (MP3) â†’ text JSON |
| Embedding | **SentenceTransformers (all-MiniLM-L6-v2)** | Encodes text chunks into semantic vectors |
| Vector Search | **FAISS (Facebook AI Similarity Search)** | Finds top-k similar chunks using cosine similarity |
| Orchestration | **Python** | Pipeline and logic |
| Environment | **VS Code / Google Colab** | Local and cloud execution |

---

## ğŸ“ Folder Structure

```
RAG BASES AI/
â”œâ”€ audios/                     # Input audio files (MP3)
â”œâ”€ jsons/                      # Whisper transcripts (.json)
â”‚   â”œâ”€ python_course_fixed_00.json
â”‚   â”œâ”€ ...
â”œâ”€ vectors.joblib              # Embeddings + text chunks (ready-to-use)
â”œâ”€ preprocess_json.py          # Converts jsons â†’ vectors.joblib
â”œâ”€ mp3_to_json.py              # Converts audios â†’ jsons (only if needed)
â”œâ”€ sanity_check.py             # Verifies vector retrieval
â”œâ”€ whisper_utils.py            # (support utils, optional)
â”œâ”€ videos/                     # (optional raw MP4s)
â”œâ”€ README.md                   # This file
â””â”€ presentation/               # PPTX or PDF slides
```

---

## ğŸ§  How It Works (Pipeline)

### 1ï¸âƒ£ Video â†’ Audio
Split long YouTube lectures into smaller 10â€“15 minute MP4 segments and extract MP3s using `ffmpeg`.

### 2ï¸âƒ£ Audio â†’ JSON Transcripts
Use Whisper (`base` for testing, `large-v2` for accuracy) to transcribe:
```bash
python mp3_to_json.py
```
Each file in `/jsons` stores text, timestamps, and metadata.

### 3ï¸âƒ£ JSON â†’ Vectors
Convert transcripts into embeddings for retrieval:
```bash
python preprocess_json.py
```
This creates `vectors.joblib` which stores embeddings and text chunks.

### 4ï¸âƒ£ Query â†’ Retrieval (Sanity Check)
Test that retrieval works using FAISS:
```bash
python sanity_check.py
```

---

## ğŸ§© Sanity Check (Retrieval Verification)

The `sanity_check.py` script:

- Loads `vectors.joblib` (embeddings + texts)  
- Builds a FAISS cosine similarity index  
- Encodes your test query (e.g. *â€œExplain Python functions with an example.â€*)  
- Finds top-5 most relevant transcript chunks  

### âœ… Example Output:
```
Top matches:

1. score=0.596  Define a function in Python using 'def'...
2. score=0.579  List comprehensions provide a concise way...
3. score=0.556  Nested functions can access variables...
4. score=0.550  The if statement allows decision making...
5. score=0.534  Here's a dice roller example...
```

**Interpretation:**  
The assistant retrieves accurate and semantically similar transcript chunks, proving that the RAG pipeline is functioning correctly.

---

## ğŸ§® Rebuilding Embeddings (Optional)

If you modify your transcripts or add new ones:
```bash
python preprocess_json.py
```
This regenerates `vectors.joblib` using SentenceTransformers.

---

## ğŸ¤– Adding an LLM Layer (Future Step)

Once retrieval works, integrate an LLM (like GPT-4, Mistral, or Llama 3) for full RAG responses.

### Example Prompt Template:
```
You are a helpful teaching assistant.
Use only the provided CONTEXT to answer.
If uncertain, say "I donâ€™t know."

QUESTION:
{user_query}

CONTEXT:
{top_k_passages}
```

Send that to your chosen LLM API or local model.

---

## ğŸ§° Installation (Local)

```bash
pip install -U openai-whisper faiss-cpu sentence-transformers joblib
```

If you need to process audio:
```bash
pip install -U ffmpeg-python
# Make sure FFmpeg is installed system-wide
```

---

## â˜ï¸ Running on Google Colab

1. Upload your project folder to Google Drive  
2. Mount drive:
```python
from google.colab import drive
drive.mount('/content/drive')
```
3. Navigate to your folder and run each step:  
   - `mp3_to_json.py` â†’ transcription  
   - `preprocess_json.py` â†’ embedding  
   - `sanity_check.py` â†’ retrieval check  
4. Download `jsons/` and `vectors.joblib` back to local once done.

---

## ğŸ§¾ Results Summary

| Metric | Description |
|--------|--------------|
| Avg. Cosine Similarity | ~0.56â€“0.78 across multiple Python topics |
| Retrieval Accuracy | High for key terms (functions, loops, classes) |
| Speed | Sub-second FAISS search on small dataset |
| Whisper Quality | Large-v2 yields ~95% accurate transcripts |

---

## ğŸ›  Fine-Tuning & Improvements

1. **Transcription:** Mix Whisper `large-v2` for key clips and `base` for speed  
2. **Chunking:** Tune chunk size (600â€“1000 chars, 100â€“200 overlap)  
3. **Embedding:** Try `text-embedding-3-small` for nuanced retrieval  
4. **Indexing:** Use FAISS IVF or HNSW for scaling large datasets  
5. **Evaluation:** Build QA pairs, track top-k accuracy and human helpfulness  

---

## ğŸ§© GitHub Upload Instructions

Before pushing, create a `.gitignore` file:
```
.venv/
__pycache__/
*.pyc
videos/
audios/
*.mp4
*.wav
*.m4a
.ipynb_checkpoints/
```

Then run:
```bash
git init
git add .
git commit -m "Capstone: RAG Teaching Assistant (Whisper large-v2)"
git branch -M main
git remote add origin https://github.com/<username>/<repo>.git
git push -u origin main
```

---

## ğŸ“š References
- [OpenAI Whisper (ASR)](https://github.com/openai/whisper)  
- [SentenceTransformers](https://www.sbert.net)  
- [FAISS by Meta AI](https://github.com/facebookresearch/faiss)  
- Bro Code Python Course (YouTube, 12 hours)  
- CloudxLab Capstone Guidelines  

---

## ğŸ‘¨â€ğŸ’» Author
**Shivam Pathak**  
Capstone Project â€” *RAG-Based AI Teaching Assistant*  
CloudxLab | 2025
